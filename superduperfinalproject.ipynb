{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning LayoutLMForTokenClassification on FUNSD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurielw05/doc_transformers/blob/main/superduperfinalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngqdEv0rP01q"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we are going to fine-tune the LayoutLM model by Microsoft Research on the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset, which is a collection of annotated form documents. The goal of our model is to learn the annotations of a number of labels (\"question\", \"answer\", \"header\" and \"other\") on those forms, such that it can be used to annotate unseen forms in the future.\n",
        "\n",
        "* Original LayoutLM paper: https://arxiv.org/abs/1912.13318\n",
        "\n",
        "* Original FUNSD paper: https://arxiv.org/abs/1905.13538\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K4S2s33ebY0"
      },
      "source": [
        "## Install libraries\n",
        "\n",
        "Currently you have to first install the `unilm` package, and then the `transformers` package (which updates the outdated `transformers` package that is included in the `unilm` package). The reason we also install the `unilm` package is because we need its preprocessing files. I've forked it, and removed some statements which introduced some issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5cngOTr6SqEf",
        "outputId": "9732fea6-c077-4a43-f334-041d11f1833a"
      },
      "source": [
        "! rm -r unilm\n",
        "! git clone -b remove_torch_save https://github.com/NielsRogge/unilm.git\n",
        "! cd unilm/layoutlm\n",
        "! pip install unilm/layoutlm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'unilm'...\n",
            "remote: Enumerating objects: 4248, done.\u001b[K\n",
            "remote: Total 4248 (delta 0), reused 0 (delta 0), pack-reused 4248\u001b[K\n",
            "Receiving objects: 100% (4248/4248), 7.29 MiB | 22.29 MiB/s, done.\n",
            "Resolving deltas: 100% (2086/2086), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./unilm/layoutlm\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting transformers==2.9.0\n",
            "  Using cached transformers-2.9.0-py3-none-any.whl (635 kB)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.7/dist-packages (from layoutlm==0.0) (2.0)\n",
            "Requirement already satisfied: lxml==4.5.1 in /usr/local/lib/python3.7/dist-packages (from layoutlm==0.0) (4.5.1)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from layoutlm==0.0) (0.0.12)\n",
            "Collecting Pillow==7.1.2\n",
            "  Using cached Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->layoutlm==0.0) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->layoutlm==0.0) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->layoutlm==0.0) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->layoutlm==0.0) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (3.7.1)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Using cached tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (7.1.2)\n",
            "Building wheels for collected packages: layoutlm\n",
            "  Building wheel for layoutlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for layoutlm: filename=layoutlm-0.0-py3-none-any.whl size=11487 sha256=e28a435fe2e2a2ae9ab3e0735ccd4a315b6d6b390223a1e9fd7866b1a1a0a067\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-owaw4tq1/wheels/ce/83/11/541e5dad69eb41732abc7eaa2efde047a655aaae9c6df0fe15\n",
            "Successfully built layoutlm\n",
            "Installing collected packages: tokenizers, transformers, Pillow, layoutlm\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.22.0.dev0\n",
            "    Uninstalling transformers-4.22.0.dev0:\n",
            "      Successfully uninstalled transformers-4.22.0.dev0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.2.0\n",
            "    Uninstalling Pillow-9.2.0:\n",
            "      Successfully uninstalled Pillow-9.2.0\n",
            "  Attempting uninstall: layoutlm\n",
            "    Found existing installation: layoutlm 0.0\n",
            "    Uninstalling layoutlm-0.0:\n",
            "      Successfully uninstalled layoutlm-0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytesseract 0.3.9 requires Pillow>=8.0.0, but you have pillow 7.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-7.1.2 layoutlm-0.0 tokenizers-0.7.0 transformers-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sszed__jQgqW",
        "outputId": "085f5a73-8056-488c-e3f2-c9924d569932"
      },
      "source": [
        "! rm -r transformers\n",
        "! git clone https://github.com/huggingface/transformers.git\n",
        "! cd transformers\n",
        "! pip install ./transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 104376, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 104376 (delta 0), reused 1 (delta 0), pack-reused 104369\u001b[K\n",
            "Receiving objects: 100% (104376/104376), 96.96 MiB | 21.54 MiB/s, done.\n",
            "Resolving deltas: 100% (77118/77118), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.22.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->transformers==4.22.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.22.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.22.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.22.0.dev0) (2022.6.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.22.0.dev0-py3-none-any.whl size=4703241 sha256=7694a9e129ad9c0d8e61b5684aa8d4b34421e529d6d3b063be9821b632f09f46\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qpuhdmhe/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.7.0\n",
            "    Uninstalling tokenizers-0.7.0:\n",
            "      Successfully uninstalled tokenizers-0.7.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.9.0\n",
            "    Uninstalling transformers-2.9.0:\n",
            "      Successfully uninstalled transformers-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "layoutlm 0.0 requires transformers==2.9.0, but you have transformers 4.22.0.dev0 which is incompatible.\u001b[0m\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.22.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGMkEG5aRB0D"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "Here we download the data of the [FUNSD dataset](https://guillaumejaume.github.io/FUNSD/) from the web. This results in a directory called \"data\" being created, which has 2 subdirectories, one for training and one for testing. Each of those has 2 subdirectories in turn, one containing the images as png files and one containing the annotations in json format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTFnEZagQm4v",
        "outputId": "3cae90e5-b2e1-4aa0-9396-6764ac32ca53"
      },
      "source": [
        "! wget https://guillaumejaume.github.io/FUNSD/dataset.zip\n",
        "! unzip dataset.zip && mv dataset data && rm -rf dataset.zip __MACOSX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-07 15:23:46--  https://guillaumejaume.github.io/FUNSD/dataset.zip\n",
            "Resolving guillaumejaume.github.io (guillaumejaume.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to guillaumejaume.github.io (guillaumejaume.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16838830 (16M) [application/zip]\n",
            "Saving to: ‘dataset.zip.1’\n",
            "\n",
            "dataset.zip.1       100%[===================>]  16.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-07 15:23:47 (108 MB/s) - ‘dataset.zip.1’ saved [16838830/16838830]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "replace dataset/training_data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrNMR64LsJXm"
      },
      "source": [
        "Let's take a look at a training example. For this, we are going to use PIL (Python Image Library)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG-eGcj3sNPs"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "image = Image.open(\"/content/data/training_data/images/0000971160.png\")\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAVffmnZyUvw"
      },
      "source": [
        "Now let's plot its corresponding annotations. Basically, if you type `data['form']`, you get a list of all general annotations. Each general annotation has a label, a bounding box, and one or more words, which in also have their own bounding box. The bounding boxes are in [xleft, ytop, xright, ybottom] format.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPKkuJQ4sdZc"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/data/training_data/annotations/0000971160.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "for annotation in data['form']:\n",
        "  print(annotation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs4L3S5a2Gfb"
      },
      "source": [
        "The PIL library has a handy ImageDraw module, which -you guessed it- allows to draw things (such as rectangles) on an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWaHFM_LtKPP"
      },
      "source": [
        "draw = ImageDraw.Draw(image, \"RGBA\")\n",
        "\n",
        "font = ImageFont.load_default()\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
        "\n",
        "for annotation in data['form']:\n",
        "  label = annotation['label']\n",
        "  general_box = annotation['box']\n",
        "  draw.rectangle(general_box, outline=label2color[label], width=2)\n",
        "  draw.text((general_box[0] + 10, general_box[1] - 10), label, fill=label2color[label], font=font)\n",
        "  words = annotation['words']\n",
        "  for word in words:\n",
        "    box = word['box']\n",
        "    draw.rectangle(box, outline=label2color[label], width=1)\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyWQNLSCRJN7"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "Next, we need to turn the document images into individual tokens and corresponding labels (BIOES format, see further). We do this both for the training and test datasets. Make sure to run this from the `/content` directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DWRyOR9RuY6"
      },
      "source": [
        "! python unilm/layoutlm/examples/seq_labeling/preprocess.py --data_dir data/training_data/annotations \\\n",
        "                                                      --data_split train \\\n",
        "                                                      --output_dir data \\\n",
        "                                                      --model_name_or_path microsoft/layoutlm-base-uncased \\\n",
        "                                                      --max_len 510\n",
        "\n",
        "! python unilm/layoutlm/examples/seq_labeling/preprocess.py --data_dir data/testing_data/annotations \\\n",
        "                                                      --data_split test \\\n",
        "                                                      --output_dir data \\\n",
        "                                                      --model_name_or_path microsoft/layoutlm-base-uncased \\\n",
        "                                                      --max_len 510"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc4Cu0ZyO5M_"
      },
      "source": [
        "Next, we create a labels.txt file that contains the unique labels of the FUNSD dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iGOU0s3UR2u"
      },
      "source": [
        "! cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9FhkG9U8yg"
      },
      "source": [
        "## Define a PyTorch dataset\n",
        "\n",
        "First, we create a list containing the unique labels based on `data/labels.txt` (run this from the content directory):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "675rRa0QXnMp"
      },
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def get_labels(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = f.read().splitlines()\n",
        "    if \"O\" not in labels:\n",
        "        labels = [\"O\"] + labels\n",
        "    return labels\n",
        "\n",
        "labels = get_labels(\"data/labels.txt\")\n",
        "num_labels = len(labels)\n",
        "label_map = {i: label for i, label in enumerate(labels)}\n",
        "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
        "pad_token_label_id = CrossEntropyLoss().ignore_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ2LGEsez2u2"
      },
      "source": [
        "We can see that the dataset uses the so-called BIOES annotation scheme to annotate the tokens. This means that a given token can be either at the beginning (B), inside (I), outside (O), at the end (E) or start (S) of a given entity. Entities include ANSWER, QUESTION, HEADER and OTHER: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-qXLkP9Yq_L"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ck0ZFfZInR"
      },
      "source": [
        "Next, we can create a PyTorch dataset and corresponding dataloader (both for training and evaluation):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUJftzeBWh2S"
      },
      "source": [
        "from transformers import LayoutLMTokenizer\n",
        "from layoutlm.data.funsd import FunsdDataset, InputFeatures\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "args = {'local_rank': -1,\n",
        "        'overwrite_cache': True,\n",
        "        'data_dir': '/content/data',\n",
        "        'model_name_or_path':'microsoft/layoutlm-base-uncased',\n",
        "        'max_seq_length': 512,\n",
        "        'model_type': 'layoutlm',}\n",
        "\n",
        "# class to turn the keys of a dict into attributes (thanks Stackoverflow)\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttrDict(args)\n",
        "\n",
        "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
        "\n",
        "# the LayoutLM authors already defined a specific FunsdDataset, so we are going to use this here\n",
        "train_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=2)\n",
        "\n",
        "eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
        "eval_sampler = SequentialSampler(eval_dataset)\n",
        "eval_dataloader = DataLoader(eval_dataset,\n",
        "                             sampler=eval_sampler,\n",
        "                            batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NMUBzgOdqu"
      },
      "source": [
        "len(train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toFjxtn71B1U"
      },
      "source": [
        "len(eval_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhINSBw9I24G"
      },
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "input_ids = batch[0][0]\n",
        "tokenizer.decode(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cEmLDoUFcm"
      },
      "source": [
        "## Define and fine-tune the model\n",
        "\n",
        "As this is a sequence labeling task, we are going to load `LayoutLMForTokenClassification` (the base sized model) from the hub. We are going to fine-tune it on a downstream task, namely FUNSD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIdOsFBiTsuw"
      },
      "source": [
        "from transformers import LayoutLMForTokenClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=num_labels)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3weFr_pz1mla"
      },
      "source": [
        "Now we can start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu0qePs2cRKo"
      },
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "global_step = 0\n",
        "num_train_epochs = 5\n",
        "t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
        "\n",
        "#put the model in training mode\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "  for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "      input_ids = batch[0].to(device)\n",
        "      bbox = batch[4].to(device)\n",
        "      attention_mask = batch[1].to(device)\n",
        "      token_type_ids = batch[2].to(device)\n",
        "      labels = batch[3].to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
        "                      labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      # print loss every 100 steps\n",
        "      if global_step % 100 == 0:\n",
        "        print(f\"Loss after {global_step} steps: {loss.item()}\")\n",
        "\n",
        "      # backward pass to get the gradients \n",
        "      loss.backward()\n",
        "\n",
        "      #print(\"Gradients on classification head:\")\n",
        "      #print(model.classifier.weight.grad[6,:].sum())\n",
        "\n",
        "      # update\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNsh67yn5XFb"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now let's evaluate on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1rNslap5Y3N"
      },
      "source": [
        "import numpy as np\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "eval_loss = 0.0\n",
        "nb_eval_steps = 0\n",
        "preds = None\n",
        "out_label_ids = None\n",
        "\n",
        "# put model in evaluation mode\n",
        "model.eval()\n",
        "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        input_ids = batch[0].to(device)\n",
        "        bbox = batch[4].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        token_type_ids = batch[2].to(device)\n",
        "        labels = batch[3].to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
        "                        labels=labels)\n",
        "        # get the loss and logits\n",
        "        tmp_eval_loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        eval_loss += tmp_eval_loss.item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # compute the predictions\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(\n",
        "                out_label_ids, labels.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "\n",
        "# compute average evaluation loss\n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "preds = np.argmax(preds, axis=2)\n",
        "\n",
        "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "\n",
        "for i in range(out_label_ids.shape[0]):\n",
        "    for j in range(out_label_ids.shape[1]):\n",
        "        if out_label_ids[i, j] != pad_token_label_id:\n",
        "            out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
        "            preds_list[i].append(label_map[preds[i][j]])\n",
        "\n",
        "results = {\n",
        "    \"loss\": eval_loss,\n",
        "    \"precision\": precision_score(out_label_list, preds_list),\n",
        "    \"recall\": recall_score(out_label_list, preds_list),\n",
        "    \"f1\": f1_score(out_label_list, preds_list),\n",
        "}\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzOwxxIHM30r"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now comes the fun part! We can now use the fine-tuned model and test it on unseen data.\n",
        "\n",
        "Note that LayoutLM relies on an external OCR engine (it's not end-to-end -> that's probably something for the future). The test data itself also contains the annotated bounding boxes, but let's run an OCR engine ourselves.\n",
        "\n",
        "So let's load in a image of the test set, run our own OCR on it to get the bounding boxes, then run LayoutLM on the individual tokens and visualize the result!\n",
        "\n",
        "Sources:\n",
        "* https://www.kaggle.com/jpmiller/layoutlm-starter\n",
        "* https://bhadreshpsavani.medium.com/how-to-use-tesseract-library-for-ocr-in-google-colab-notebook-5da5470e4fe0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgrZ8pENAZHD"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_7dmPC64Dfd"
      },
      "source": [
        "import pytesseract\n",
        "\n",
        "#image = Image.open('/content/form_example.jpg')\n",
        "image = Image.open(\"/content/data/training_data/images/Screenshot_343.png\")\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExOidmxszn6t"
      },
      "source": [
        "Here we run Tesseract (an OCR engine built by Google) on the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPJlJJX0AYE7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "width, height = image.size\n",
        "w_scale = 1000/width\n",
        "h_scale = 1000/height\n",
        "\n",
        "ocr_df = pytesseract.image_to_data(image, output_type='data.frame') \\\n",
        "            \n",
        "ocr_df = ocr_df.dropna() \\\n",
        "               .assign(left_scaled = ocr_df.left*w_scale,\n",
        "                       width_scaled = ocr_df.width*w_scale,\n",
        "                       top_scaled = ocr_df.top*h_scale,\n",
        "                       height_scaled = ocr_df.height*h_scale,\n",
        "                       right_scaled = lambda x: x.left_scaled + x.width_scaled,\n",
        "                       bottom_scaled = lambda x: x.top_scaled + x.height_scaled)\n",
        "\n",
        "float_cols = ocr_df.select_dtypes('float').columns\n",
        "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "ocr_df[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozxEGUSdGkkH"
      },
      "source": [
        "len(ocr_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgtCwrvK3k2W"
      },
      "source": [
        "Here we create a list of words, actual bounding boxes, and normalized boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoB1uEtOP_2H"
      },
      "source": [
        "words = list(ocr_df.text)\n",
        "coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "actual_boxes = []\n",
        "for idx, row in coordinates.iterrows():\n",
        "  x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
        "  actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+widght, top+height) to get the actual box \n",
        "  actual_boxes.append(actual_box)\n",
        "\n",
        "def normalize_box(box, width, height):\n",
        "    return [\n",
        "        int(1000 * (box[0] / width)),\n",
        "        int(1000 * (box[1] / height)),\n",
        "        int(1000 * (box[2] / width)),\n",
        "        int(1000 * (box[3] / height)),\n",
        "    ]\n",
        "\n",
        "boxes = []\n",
        "for box in actual_boxes:\n",
        "  boxes.append(normalize_box(box, width, height))\n",
        "boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfneTqU875U1"
      },
      "source": [
        "This should become the future API of LayoutLMTokenizer (`prepare_for_model()`): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDEh152kB-LE"
      },
      "source": [
        "def convert_example_to_features(image, words, boxes, actual_boxes, tokenizer, args, cls_token_box=[0, 0, 0, 0],\n",
        "                                 sep_token_box=[1000, 1000, 1000, 1000],\n",
        "                                 pad_token_box=[0, 0, 0, 0]):\n",
        "      width, height = image.size\n",
        "\n",
        "      wordss = []\n",
        "      tokens = []\n",
        "      token_boxes = []\n",
        "      actual_bboxes = [] # we use an extra b because actual_boxes is already used\n",
        "      token_actual_boxes = []\n",
        "      for word, box, actual_bbox in zip(words, boxes, actual_boxes):\n",
        "          word_tokens = tokenizer.tokenize(word)\n",
        "          tokens.extend(word_tokens)\n",
        "          token_boxes.extend([box] * len(word_tokens))\n",
        "          actual_bboxes.extend([actual_bbox] * len(word_tokens))\n",
        "          token_actual_boxes.extend([actual_bbox] * len(word_tokens))\n",
        "          wordss.append(word)\n",
        "          \n",
        "\n",
        "      # Truncation: account for [CLS] and [SEP] with \"- 2\". \n",
        "      special_tokens_count = 2 \n",
        "      if len(tokens) > args.max_seq_length - special_tokens_count:\n",
        "          tokens = tokens[: (args.max_seq_length - special_tokens_count)]\n",
        "          token_boxes = token_boxes[: (args.max_seq_length - special_tokens_count)]\n",
        "          actual_bboxes = actual_bboxes[: (args.max_seq_length - special_tokens_count)]\n",
        "          token_actual_boxes = token_actual_boxes[: (args.max_seq_length - special_tokens_count)]\n",
        "\n",
        "      # add [SEP] token, with corresponding token boxes and actual boxes\n",
        "      tokens += [tokenizer.sep_token]\n",
        "      token_boxes += [sep_token_box]\n",
        "      actual_bboxes += [[0, 0, width, height]]\n",
        "      token_actual_boxes += [[0, 0, width, height]]\n",
        "      \n",
        "      segment_ids = [0] * len(tokens)\n",
        "\n",
        "      # next: [CLS] token\n",
        "      tokens = [tokenizer.cls_token] + tokens\n",
        "      token_boxes = [cls_token_box] + token_boxes\n",
        "      actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n",
        "      token_actual_boxes = [[0, 0, width, height]] + token_actual_boxes\n",
        "      segment_ids = [1] + segment_ids\n",
        "\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "      # tokens are attended to.\n",
        "      input_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = args.max_seq_length - len(input_ids)\n",
        "      input_ids += [tokenizer.pad_token_id] * padding_length\n",
        "      input_mask += [0] * padding_length\n",
        "      segment_ids += [tokenizer.pad_token_id] * padding_length\n",
        "      token_boxes += [pad_token_box] * padding_length\n",
        "      token_actual_boxes += [pad_token_box] * padding_length\n",
        "\n",
        "      assert len(input_ids) == args.max_seq_length\n",
        "      assert len(input_mask) == args.max_seq_length\n",
        "      assert len(segment_ids) == args.max_seq_length\n",
        "      #assert len(label_ids) == args.max_seq_length\n",
        "      assert len(token_boxes) == args.max_seq_length\n",
        "      assert len(token_actual_boxes) == args.max_seq_length\n",
        "      \n",
        "      return input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes, wordss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omze6dqfUTH-"
      },
      "source": [
        "input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes, wordss = convert_example_to_features(image=image, words=words, boxes=boxes, actual_boxes=actual_boxes, tokenizer=tokenizer, args=args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordss)"
      ],
      "metadata": {
        "id": "IYsnAglDjtKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_actual_boxes"
      ],
      "metadata": {
        "id": "PcqYqacR1-pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oE3rFL2WJDB"
      },
      "source": [
        "tokenizer.decode(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEM6O-kF55d1"
      },
      "source": [
        "Now let's perform a forward pass!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QrrjzMA6Xps"
      },
      "source": [
        "input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "input_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZSUel096fFd"
      },
      "source": [
        "attention_mask = torch.tensor(input_mask, device=device).unsqueeze(0)\n",
        "attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyux4BmV6jEw"
      },
      "source": [
        "token_type_ids = torch.tensor(segment_ids, device=device).unsqueeze(0)\n",
        "token_type_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoicSudS6p2m"
      },
      "source": [
        "bbox = torch.tensor(token_boxes, device=device).unsqueeze(0)\n",
        "bbox.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y-_7Wqb51U9"
      },
      "source": [
        "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLcMfJH26ozs"
      },
      "source": [
        "outputs.logits.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dBXbhdJnz4"
      },
      "source": [
        "outputs.logits.argmax(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePeWryCEzvYW"
      },
      "source": [
        "Of course, LayoutLM outputs labels at the token level, but we are interested in the predicted labels at the word level. So we should actually only incorporate the predicted labels of tokens that are the first token of a given word: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_actual_boxes"
      ],
      "metadata": {
        "id": "HoxlFTNDi8x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I5CiV1WM2sW"
      },
      "source": [
        "token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
        "\n",
        "word_level_predictions = [] # let's turn them into word level predictions\n",
        "final_boxes = []\n",
        "for id, token_pred, box in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes):\n",
        "  if (tokenizer.decode([id]).startswith(\"##\")) or (id in [tokenizer.cls_token_id, \n",
        "                                                           tokenizer.sep_token_id, \n",
        "                                                          tokenizer.pad_token_id]):\n",
        "    # skip prediction + bounding box\n",
        "\n",
        "    continue\n",
        "  else:\n",
        "    word_level_predictions.append(token_pred)\n",
        "    final_boxes.append(box)\n",
        "\n",
        "# for id, prediction in zip(input_ids.squeeze().tolist(), predictions):\n",
        "#   if id != 0:\n",
        "#     print(tokenizer.decode([id]), label_map[prediction])\n",
        "print(word_level_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5W_2CxhOzzM"
      },
      "source": [
        "print(len(word_level_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WQaKPBnVl-L"
      },
      "source": [
        "print(len(final_boxes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Vishnunkumar/doc_transformers.git"
      ],
      "metadata": {
        "id": "7wZzpj1x3qbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q doc-transformers"
      ],
      "metadata": {
        "id": "dBI0q3vc3q-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html"
      ],
      "metadata": {
        "id": "37ME5yRw3rOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PIL\n",
        "!pip install Pillow"
      ],
      "metadata": {
        "id": "vcSkCSpS3xoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from doc_transformers import form_parser\n",
        "\n",
        "# gets the bounding boxes, predictions, extracted words and image processed\n",
        "bbox, preds, words, image = form_parser.process_image(image)\n",
        "\n",
        "# returns image and extracted key-value pairs along with title as the output\n",
        "im, df = form_parser.visualize_image(bbox, preds, words, image)\n",
        "print(df)\n",
        "# process and returns k-v pairs by concatenating relevant strings.\n",
        "df_main = form_parser.process_form(df)"
      ],
      "metadata": {
        "id": "Hh7OAvIj35k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIOIgtq_Mzie"
      },
      "source": [
        "Let's visualize the result!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm3sGnBsL64o"
      },
      "source": [
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "font = ImageFont.load_default()\n",
        "\n",
        "def iob_to_label(label):\n",
        "  if label != 'O':\n",
        "    return label[2:]\n",
        "  else:\n",
        "    return \"other\"\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
        "SimpleData = []\n",
        "for prediction, box, word in zip(word_level_predictions, final_boxes, wordss):\n",
        "    SimpleData.append([label_map[prediction], word])\n",
        "    predicted_label = iob_to_label(label_map[prediction]).lower()\n",
        "    draw.rectangle(box, outline=label2color[predicted_label])\n",
        "    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
        "print(SimpleData)\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg7C1_PReCXU"
      },
      "source": [
        "Compare this to the ground truth:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyh19IBHwy_x"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/data/testing_data/annotations/83443897.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "for annotation in data['form']:\n",
        "  print(annotation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TtA3buU0kDZ"
      },
      "source": [
        "Note that we only print the labels for the general bounding boxes, for readability. Each general bounding box consists of one or more bounding boxes, which comprise the individual words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcInpWH4K-V5"
      },
      "source": [
        "image = Image.open(\"/content/data/testing_data/images/83443897.png\")\n",
        "image = image.convert('RGB')\n",
        "\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
        "\n",
        "for annotation in data['form']:\n",
        "  label = annotation['label']\n",
        "  general_box = annotation['box']\n",
        "  draw.rectangle(general_box, outline=label2color[label], width=2)\n",
        "  draw.text((general_box[0] + 10, general_box[1] - 10), label, fill=label2color[label], font=font)\n",
        "  words = annotation['words']\n",
        "  for word in words:\n",
        "    box = word['box']\n",
        "    draw.rectangle(box, outline=label2color[label], width=1)\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKb0UAkSlizR"
      },
      "source": [
        "## Legacy\n",
        "\n",
        "The code below was used during the development of this notebook, but is not used anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7bNPb0IvaYu"
      },
      "source": [
        "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# # Prepare optimizer and schedule (linear warmup and decay)\n",
        "# no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#             p\n",
        "#             for n, p in model.named_parameters()\n",
        "#             if not any(nd in n for nd in no_decay)\n",
        "#         ],\n",
        "#         \"weight_decay\": 0.0,\n",
        "#     },\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#             p\n",
        "#             for n, p in model.named_parameters()\n",
        "#             if any(nd in n for nd in no_decay)\n",
        "#         ],\n",
        "#         \"weight_decay\": 0.0,\n",
        "#     },\n",
        "# ]\n",
        "\n",
        "# optimizer = AdamW(\n",
        "#     optimizer_grouped_parameters, lr=5e-5, eps=1e-8\n",
        "# )\n",
        "\n",
        "# scheduler = get_linear_schedule_with_warmup(\n",
        "#     optimizer, num_warmup_steps=0, num_training_steps=-1\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}